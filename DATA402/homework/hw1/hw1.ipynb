{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) $Y =  \\beta_1X + \\beta_0 + \\epsilon$\n",
    "\n",
    "(b) $Y =  \\beta*X + 5000 + \\epsilon$\n",
    "\n",
    "(c) $Y =  \\beta*X^2 + \\epsilon$\n",
    "\n",
    "(d) $Y =  log(\\beta*X) + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derive the Least Squares Estimator(s) for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(a)\n",
    "- Residual: $\\epsilon = Y - \\beta_1X - \\beta_0$\n",
    "- Minimize Sum of Squared Residual (loss): $l = \\sum_{i=1}^{n} (Y_i - (\\beta_1X_i + \\beta_0))^2$\n",
    "- Take partial derivatives with respect to $\\beta_0$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_0} = -2\\sum_{i=1}^{n} (Y_i - \\beta_1X_i - \\beta_0) = 0$\n",
    "    - $\\sum_{i=1}^{n} Y_i = \\beta_1\\sum_{i=1}^{n} X_i + n\\beta_0$\n",
    "    - $\\beta_0 = \\frac{1}{n}(\\sum_{i=1}^{n} Y_i - \\beta_1\\sum_{i=1}^{n} X_i)$\n",
    "- Take $\\bar{X}$ and $\\bar{Y}$ as sample means of $X$ and $Y$ respectively.\n",
    "- Solving for $\\beta_0$ gives:\n",
    "    - $\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = -2\\sum_{i=1}^{n} X_i(Y_i - \\beta_1X_i - \\beta_0) = 0$\n",
    "- Simplify:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + \\beta_0\\sum_{i=1}^{n} X_i$\n",
    "- Substitute $\\beta_0$ into equation:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + (\\bar{Y} - \\beta_1\\bar{X})\\sum_{i=1}^{n} X_i$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\sum_{i=1}^n X_i Y_i - \\bar{Y} \\sum_{i=1}^n X_i = \\beta_1 \\left( \\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i \\right)$\n",
    "- Finally:\n",
    "    - $\\beta_1 = \\frac{\\sum_{i=1}^n X_i Y_i - \\bar{Y} \\sum_{i=1}^n X_i}{\\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "- $\\beta_0 = 5000$\n",
    "- All previous steps remain the same for $\\beta_1$.\n",
    "- Substitute $\\beta_0$ into equation:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + 5000\\sum_{i=1}^{n} X_i$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\sum_{i=1}^n X_i Y_i - 5000 \\sum_{i=1}^n X_i = \\beta_1 \\left( \\sum_{i=1}^n X_i^2 \\right)$\n",
    "- Finally:\n",
    "    - $\\beta_1 = \\frac{\\sum_{i=1}^n X_i Y_i - 5000 \\sum_{i=1}^n X_i}{\\sum_{i=1}^n X_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "- $\\beta_0 = 0$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = -2\\sum_{i=1}^{n} X_i^2(Y_i - \\beta_1X_i^2) = 0$\n",
    "- Simplify:\n",
    "    - $\\sum_{i=1}^{n} X_i^2Y_i = \\beta_1\\sum_{i=1}^{n} X_i^4$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\beta_1 = \\frac{\\sum_{i=1}^n X_i^2 Y_i}{\\sum_{i=1}^n X_i^4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "- $\\beta_0 = 0$\n",
    "- Rewrite as:\n",
    "    - $Y = \\beta_1log(X) + \\epsilon$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = -2\\sum_{i=1}^{n} Y_i - log(\\beta_1) - log(X_i) = 0$\n",
    "- Solve for $log(\\beta_1)$:\n",
    "    - $log(\\beta_1) = \\frac{\\sum_{i=1}^n Y_i - log(X_i)}{n}$\n",
    "- Finally solve for $\\beta_1$:\n",
    "    - $\\beta_1 = e^{\\frac{\\sum_{i=1}^n Y_i - log(X_i)}{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derive the Method of Moments Estimator(s) for each model. You do *not* need to prove that the mean of observations is the UMVUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    "- Residual: $\\epsilon = Y - \\beta_1X - \\beta_0$\n",
    "- Take the expectation of the residual:\n",
    "    - $E[\\epsilon] = E[Y] - \\beta_1E[X] - \\beta_0$\n",
    "- Since $E[\\epsilon] = 0$:\n",
    "    - $E[Y] = \\beta_1E[X] + \\beta_0$\n",
    "- Estimate expectations using sample means:\n",
    "    - $\\bar{Y} = \\beta_1\\bar{X} + \\beta_0$\n",
    "- Solve for $\\beta_0$:\n",
    "    - $\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}$\n",
    "- Take the expectation of $Y$:\n",
    "    - $E[Y] = \\beta_1E[X] + \\beta_0$\n",
    "- Second moment is based on covariance:\n",
    "    - $Cov(X, Y) = \\beta_1Var(X)$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\hat{\\beta_1} = \\frac{Cov(X, Y)}{Var(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "- $\\beta_0 = 5000$\n",
    "- All previous steps remain the same for $\\beta_1.\n",
    "- Estimate expectations using sample means:\n",
    "    - $\\bar{Y} = \\beta_1\\bar{X} + 5000$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\hat{\\beta_1} = \\frac{\\bar{Y} - 5000}{\\bar{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "- $\\beta_0 = 0$\n",
    "- Take the expectation of the residual:\n",
    "    - $E[\\epsilon] = E[Y] - \\beta_1E[X^2]$\n",
    "- Since $E[\\epsilon] = 0$:\n",
    "    - $E[Y] = \\beta_1E[X^2]$\n",
    "- Estimate expectations using sample means:\n",
    "    - $\\bar{Y} = \\hat{\\beta_1}\\bar{X^2}$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\hat{\\beta_1} = \\frac{\\bar{Y}}{\\bar{X^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "- $\\beta_0 = 0$\n",
    "- Expectation of Y:\n",
    "    - $E[Y] = E[log(\\beta X) + \\epsilon]$\n",
    "    - $E[Y] = E[log(\\beta) + log(X) + \\epsilon]$\n",
    "- Since $E[\\epsilon] = 0$:\n",
    "    - $E[Y] = log(\\beta) + E[log(X)]$\n",
    "- Estimate expectations using sample means:\n",
    "    - $\\bar{Y} = log(\\hat{\\beta}) + \\bar{X}$\n",
    "- Solve for $\\beta$:\n",
    "    - $\\hat{\\beta} = e^{\\bar{Y} - \\bar{X}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Derive the Maximum Likelihood Estimator(s) for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) $Y =  \\beta_1X + \\beta_0 + \\epsilon$\n",
    "- Likelihood function: $L(\\beta_0, \\beta_1) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left(-\\frac{(Y_i - \\beta_1X_i - \\beta_0)^2}{2\\sigma^2}\\right)$\n",
    "- Log-likelihood function: $l(\\beta_0, \\beta_1) = -\\frac{n}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (Y_i - \\beta_1X_i - \\beta_0)^2$\n",
    "- Take partial derivatives with respect to $\\beta_0$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_0} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} (Y_i - \\beta_1X_i - \\beta_0) = 0$\n",
    "    - $\\sum_{i=1}^{n} Y_i = \\beta_1\\sum_{i=1}^{n} X_i + n\\beta_0$\n",
    "    - $\\beta_0 = \\frac{1}{n}(\\sum_{i=1}^{n} Y_i - \\beta_1\\sum_{i=1}^{n} X_i)$\n",
    "- Take $\\bar{X}$ and $\\bar{Y}$ as sample means of $X$ and $Y$ respectively.\n",
    "- Solving for $\\beta_0$ gives:\n",
    "    - $\\beta_0 = \\bar{Y} - \\hat{\\beta_1}\\bar{X}$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} X_i(Y_i - \\beta_1X_i - \\beta_0) = 0$\n",
    "- Simplify:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + \\beta_0\\sum_{i=1}^{n} X_i$\n",
    "- Substitute $\\beta_0$ into equation:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + (\\bar{Y} - \\beta_1\\bar{X})\\sum_{i=1}^{n} X_i$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\sum_{i=1}^n X_i Y_i - \\bar{Y} \\sum_{i=1}^n X_i = \\beta_1 \\left( \\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i \\right)$\n",
    "- Finally:\n",
    "    - $\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n X_i Y_i - \\bar{Y} \\sum_{i=1}^n X_i}{\\sum_{i=1}^n X_i^2 - \\bar{X} \\sum_{i=1}^n X_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "- $\\beta_0 = 5000$\n",
    "- All previous steps remain the same for $\\beta_1.\n",
    "- Substitute $\\beta_0$ into equation:\n",
    "    - $\\sum_{i=1}^{n} X_iY_i = \\beta_1\\sum_{i=1}^{n} X_i^2 + 5000\\sum_{i=1}^{n} X_i$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\sum_{i=1}^n X_i Y_i - 5000 \\sum_{i=1}^n X_i = \\beta_1 \\left( \\sum_{i=1}^n X_i^2 \\right)$\n",
    "- Finally:\n",
    "    - $\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n X_i Y_i - 5000 \\sum_{i=1}^n X_i}{\\sum_{i=1}^n X_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "- $\\beta_0 = 0$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} X_i^2(Y_i - \\beta_1X_i^2) = 0$\n",
    "- Simplify:\n",
    "    - $\\sum_{i=1}^{n} X_i^2Y_i = \\beta_1\\sum_{i=1}^{n} X_i^4$\n",
    "- Solve for $\\beta_1$:\n",
    "    - $\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n X_i^2 Y_i}{\\sum_{i=1}^n X_i^4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "- $\\beta_0 = 0$\n",
    "- Rewrite as:\n",
    "    - $Y = \\beta_1log(X) + \\epsilon$\n",
    "- Take partial derivatives with respect to $\\beta_1$ and set them to zero.\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} Y_i - log(\\beta_1) - log(X_i) = 0$\n",
    "- Solve for $log(\\beta_1)$:\n",
    "    - $log(\\hat{\\beta_1}) = \\frac{\\sum_{i=1}^n Y_i - log(X_i)}{n}$\n",
    "- Finally solve for $\\beta_1$:\n",
    "    - $\\hat{\\beta_1} = e^{\\frac{\\sum_{i=1}^n Y_i - log(X_i)}{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Lot Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215000</td>\n",
       "      <td>31770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105000</td>\n",
       "      <td>11622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172000</td>\n",
       "      <td>14267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244000</td>\n",
       "      <td>11160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189900</td>\n",
       "      <td>13830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalePrice  Lot Area\n",
       "0     215000     31770\n",
       "1     105000     11622\n",
       "2     172000     14267\n",
       "3     244000     11160\n",
       "4     189900     13830"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('AmesHousing.csv')\n",
    "data = data[['SalePrice', 'Lot Area']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use either the LinearRegression function in scikit-learn to fit the model from Part A. Do not do any cross-validation or tuning for this; simply fit the model on the full data, and print out the coefficient(s) that were estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 153373.89290717372\n",
      "Slope: [2.70224462]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(data[['Lot Area']], data['SalePrice'])\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function that estimates the coefficients by random choice and brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slr_guess_and_check(x, y, num_candidates=1000):\n",
    "    min_loss = float('inf')\n",
    "    for _ in range(num_candidates):\n",
    "        b0 = np.random.uniform(1000, 200000)\n",
    "        b1 = np.random.uniform(-10, 10)\n",
    "        yhat = b0 + b1*x\n",
    "        loss = np.mean((y - yhat)**2)\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            best_b0 = b0\n",
    "            best_b1 = b1\n",
    "    return best_b0, best_b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply slr_guess_and_check to the AMES Housing data. Compare the results to the “correct” coefficients that you got in B1. Make a plot, table, or summary statement to describe how close your function tends to get to the right answers. (Since your function uses randomization, you will need to run it many times to comment on how close it “tends” to get.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 148741.56333684328\n",
      "Slope: 2.7793548532319825\n"
     ]
    }
   ],
   "source": [
    "b0, b1 = slr_guess_and_check(data['Lot Area'], data['SalePrice'])\n",
    "print('Intercept:', b0)\n",
    "print('Slope:', b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | $\\beta_0$ | $\\beta_1$ |\n",
    "|-------|-----------|-----------|\n",
    "| A     | 153373.89290717372       | 2.70224462       |\n",
    "| B     | 148741.56333684328    | 2.7793548532319825       |\n",
    "\n",
    "The function tends to get very close to the right answers after running multiple times. It overshoots sometimes but stays in the right range for both $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now write a function that \"updates\" b0 and b1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slr_improvement_method(x, y, num_iterations=1000):\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    yhat = b0 + b1*x\n",
    "    loss = np.mean((y - yhat)**2)\n",
    "    for _ in range(num_iterations):\n",
    "        b0_c = np.random.uniform(1000, 200000)\n",
    "        yhat = b0_c + b1*x\n",
    "        loss_c = np.mean((y - yhat)**2)\n",
    "        if loss_c < loss:\n",
    "            b0 = b0_c\n",
    "            loss = loss_c\n",
    "        b1_c = np.random.uniform(-10, 10)\n",
    "        yhat = b0 + b1_c*x\n",
    "        loss_c = np.mean((y - yhat)**2)\n",
    "        if loss_c < loss:\n",
    "            b1 = b1_c\n",
    "            loss = loss_c\n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 154324.68369496462\n",
      "Slope: 2.6279768052587453\n"
     ]
    }
   ],
   "source": [
    "b0, b1 = slr_improvement_method(data['Lot Area'], data['SalePrice'])\n",
    "print('Intercept:', b0)\n",
    "print('Slope:', b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | $\\beta_0$ | $\\beta_1$ |\n",
    "|-------|-----------|-----------|\n",
    "| A     | 153373.89290717372       | 2.70224462       |\n",
    "| B     | 153380.35319410317    | 2.7070442508083747       |\n",
    "\n",
    "This function is very accurate and stays in a very tight range around the correct values for both $\\beta_0$ and $\\beta_1$. Even after running multiple times there are no significant deviations from the correct values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plug in appropriate summary statistics from the AMES data to get the least squares estimators for the three models in Section A. Are they similar, or not? Give an intuitive explanation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Intercept: 153373.89290717372\n",
      "Slope: 2.7022446157281115\n",
      "\n",
      "Model 2:\n",
      "Intercept: 5000\n",
      "Slope: 11.824643640102646\n",
      "\n",
      "Model 3:\n",
      "Intercept: 0\n",
      "Slope: -0.09872158273462628\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Y = B1X + B0\n",
    "x = data['Lot Area']\n",
    "y = data['SalePrice']\n",
    "b1 = np.sum(x*y) - np.mean(y)*np.sum(x)\n",
    "b1 /= np.sum(x**2) - np.mean(x)*np.sum(x)\n",
    "b0 = np.mean(y) - b1*np.mean(x)\n",
    "print('Model 1:')\n",
    "print('Intercept:', b0)\n",
    "print('Slope:', b1)\n",
    "print()\n",
    "\n",
    "# Model 2: Y = BX + 5000\n",
    "b1 = np.sum(x*y) - 5000*np.sum(x)\n",
    "b1 /= np.sum(x**2)\n",
    "print('Model 2:')\n",
    "print('Intercept:', 5000)\n",
    "print('Slope:', b1)\n",
    "print()\n",
    "\n",
    "# Model 3: Y = BX^2\n",
    "b1 = np.sum(x**2*y)\n",
    "b1 /= np.sum(x**4)\n",
    "print('Model 3:')\n",
    "print('Intercept:', 0)\n",
    "print('Slope:', b1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first model is a simple linear regression, the least squares estimator is the same as from the results from Section B. However for the second and third model they are not similar to Model since the intercept has been hardcoded as 5000 and this affects calculating $\\beta_1$. The third model is a quadratic model and the least squares estimator is extremely different from the other 2 models and this is because the model is not linear and the relationship between $X$ and $Y$ is not linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider models (a) and (b) from Section A. Mathematically, which one will have the smaller optimized squared error loss? Why? Do you expect this to be true for any chosen loss function?\n",
    "\n",
    "Model (a) will have a smaller optimized squared error loss because the intercept is not hardcoded and the model is more flexible. Model (b) has a fixed intercept of 5000 and this will not be able to capture the relationship between $X$ and $Y$ as well as Model (a). Having 2 free parameters will allow model (a) to fit better thus having a smaller optimized squared errors loss. This is true for the squared error loss function but may not be true for other loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss your summaries/plots from B3 and B5. How did these two (somewhat silly) approaches to estimation differ? Why?\n",
    "\n",
    "The brute force approach with random choice was not accurate and differed greatly than the improved brute force approach. The difference was that improved approach had a way to update the coefficients based on the error and keep minimizing the error. The brute force approach was random and did not have a way to update the coefficients based on the error and thus was not accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following problems, consider a possible loss function that we could use instead of squared error loss. For each, start the process of deriving the estimator for the model $Y = \\beta*X + \\epsilon$ . When you hit a reason it is impossible to continue, stop, and then describe in words why you were not able to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. $l(\\beta) = \\max_{i} |Y_i - \\hat{Y_i}|$\n",
    "- The loss function is not differentiable and so the derivative cannot be taken and set to zero to find the estimator for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. $l(\\beta) = \\sum_{i=1}^{n} |Y_i - \\hat{Y_i}|$\n",
    "- The loss function is not differentiable and so the derivative cannot be taken and set to zero to find the estimator for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. $l(\\beta) = \\sum_{i=1}^{n} Y_i*log(\\hat{Y_i})$\n",
    "- Take the partial derivative with respect to $\\beta$:\n",
    "    - $\\frac{\\partial l}{\\partial \\beta} = \\sum_{i=1}^{n} Y_i*\\frac{1}{\\hat{Y_i}}*\\frac{\\partial \\hat{Y_i}}{\\partial \\beta}$\n",
    "- The derivative of $log(\\hat{Y_i})$ is not defined and so the derivative cannot be taken and set to zero to find the estimator for $\\beta$.\n",
    "- It is not defined because $log(\\hat{Y_i})$ is not differentiable at 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
