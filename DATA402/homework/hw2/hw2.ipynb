{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Ishaan Sathaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following model, where $Y$ represents the price of a house and $X$ represents its size in square feet.\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ridge penalty: Sq Error + $\\lambda(\\beta_0^2 + \\beta_1^2)$ Derive the Ridge Regression estimators for $\\beta_0$ and $\\beta_1$ in terms of $\\lambda$.\n",
    "- Simplify Loss Function:\n",
    "    - $l(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)^2 + \\lambda(\\beta_0^2 + \\beta_1^2)$\n",
    "    - $l(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i^2 - 2y_i\\beta_0 - 2y_i\\beta_1x_i + \\beta_0^2 + 2\\beta_0\\beta_1x_i + \\beta_1^2x_i^2) + \\lambda(\\beta_0^2 + \\beta_1^2)$\n",
    "- Take Partial Derivate with respect to $\\beta_0$ first:\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_0} = -2\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i) + 2\\lambda\\beta_0$\n",
    "    - $0 = -2\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i) + 2\\lambda\\beta_0$\n",
    "    - $\\lambda\\beta_0 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)$\n",
    "- Take Partial Derivate with respect to $\\beta_1$ next:\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = -2\\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i) + 2\\lambda\\beta_1$\n",
    "    - $0 = -2\\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i) + 2\\lambda\\beta_1$\n",
    "    - $\\lambda\\beta_1 = \\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Solve this system of equations:\n",
    "    - $\\lambda\\beta_0 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)$\n",
    "    - $\\lambda\\beta_1 = \\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i)$\n",
    "- Simplify first equation:\n",
    "    - $n\\beta_0 + \\beta_1\\sum_{i=1}^{n}x_i = \\sum_{i=1}^{n}y_i - \\lambda\\beta_0$\n",
    "    - $(n + \\lambda) \\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i$\n",
    "- Simplify second equation:\n",
    "    - $\\beta_0\\sum_{i=1}^{n}x_i + \\beta_1\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i - \\lambda\\beta_1$\n",
    "    - $\\beta_0 \\sum_{i=1}^n x_i + (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 = \\sum_{i=1}^n x_i y_i$\n",
    "- Again, solve this system of equations:\n",
    "    - $(n + \\lambda) \\beta_0 + \\beta_1 \\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i$\n",
    "    - $\\beta_0 \\sum_{i=1}^n x_i + (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 = \\sum_{i=1}^n x_i y_i$\n",
    "- Solve for $\\beta_0$:\n",
    "    - $\\beta_0 = \\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n + \\lambda}$\n",
    "- Solve for $\\beta_1$ by substituting equation for $\\beta_0$:\n",
    "    - $\\frac{\\sum_{i=1}^n y_i - \\beta_1 \\sum_{i=1}^n x_i}{n + \\lambda} \\sum_{i=1}^n x_i + (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 = \\sum_{i=1}^n x_i y_i$\n",
    "- Multiply by $n + \\lambda$ and expand:\n",
    "    - $\\sum_{i=1}^n y_i \\sum_{i=1}^n x_i - \\beta_1 (\\sum_{i=1}^n x_i)^2 + (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 (n + \\lambda) = \\sum_{i=1}^n x_i y_i (n + \\lambda)$\n",
    "- Combine:\n",
    "    - $\\sum_{i=1}^n y_i \\sum_{i=1}^n x_i - \\beta_1 (\\sum_{i=1}^n x_i)^2 + (n + \\lambda) (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 = \\sum_{i=1}^n x_i y_i (n + \\lambda)$\n",
    "    - $\\beta_1 (\\sum_{i=1}^n x_i)^2 + (n + \\lambda) (\\sum_{i=1}^n x_i^2 + \\lambda) \\beta_1 = \\sum_{i=1}^n x_i y_i (n + \\lambda) - \\sum_{i=1}^n y_i \\sum_{i=1}^n x_i$\n",
    "- Finally, solve for $\\beta_1$:\n",
    "    - $\\beta_1 = \\frac{\\sum_{i=1}^n x_i y_i (n + \\lambda) - \\sum_{i=1}^n y_i \\sum_{i=1}^n x_i}{(\\sum_{i=1}^n x_i)^2 + (n + \\lambda) (\\sum_{i=1}^n x_i^2 + \\lambda)}$\n",
    "- Finally, substitute equation for $\\beta_1$ into equation for $\\beta_0$:\n",
    "    - $\\beta_0 = \\frac{\\sum_{i=1}^n y_i - \\frac{\\sum_{i=1}^n x_i y_i (n + \\lambda) - \\sum_{i=1}^n y_i \\sum_{i=1}^n x_i}{(\\sum_{i=1}^n x_i)^2 + (n + \\lambda) (\\sum_{i=1}^n x_i^2 + \\lambda)} \\sum_{i=1}^n x_i}{n + \\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LASSO penalty: Sq Error + $\\lambda(|\\beta_0| + |\\beta_1|)$ Derive the LASSO Regression estimators for $\\beta_0$ and $\\beta_1$ in terms of $\\lambda$, assuming that both $\\beta_0$ and $\\beta_1$ are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simplify Loss Function:\n",
    "    - $l(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)^2 + \\lambda(|\\beta_0| + |\\beta_1|)$\n",
    "    - $l(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i^2 - 2y_i\\beta_0 - 2y_i\\beta_1x_i + \\beta_0^2 + 2\\beta_0\\beta_1x_i + \\beta_1^2x_i^2) + \\lambda(|\\beta_0| + |\\beta_1|)$\n",
    "- Take Partial Derivative with respect to $\\beta_0$:\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_0} = -2\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i) + \\lambda$\n",
    "    - $0 = -2\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i) + \\lambda$\n",
    "    - $\\frac{\\lambda}{2} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)$\n",
    "- Take Partial Derivative with respect to $\\beta_1$:\n",
    "    - $\\frac{\\partial l}{\\partial \\beta_1} = -2\\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i) + \\lambda$\n",
    "    - $0 = -2\\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i) + \\lambda$\n",
    "    - $\\frac{\\lambda}{2} = \\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i)$\n",
    "- Solve this system of equations:\n",
    "    - $\\frac{\\lambda}{2} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)$\n",
    "    - $\\frac{\\lambda}{2} = \\sum_{i=1}^{n} x_i(y_i - \\beta_0 - \\beta_1x_i)$\n",
    "- Solve for $\\beta_0$:\n",
    "    - $\\sum_{i=1}^{n} y_i - \\beta_0n - \\beta_1 \\sum_{i=1}^{n} x_i = \\frac{\\lambda}{2}$\n",
    "    - $\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2}}{n}$\n",
    "- Solve for $\\beta_1$ by substituting equation for $\\beta_0$:\n",
    "    - $\\sum_{i=1}^{n} x_i y_i - \\beta_0 \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 = \\frac{\\lambda}{2}$\n",
    "    - $\\sum_{i=1}^{n} x_i y_i - \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2}}{n} \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 = \\frac{\\lambda}{2}$\n",
    "- Multiply by $n$ and expand:\n",
    "    - $n \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 n = \\frac{\\lambda}{2} n$\n",
    "- Combine:\n",
    "    - $n \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 n = \\frac{\\lambda}{2} n$\n",
    "    - $\\beta_1 \\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} x_i + \\beta_1 \\sum_{i=1}^{n} x_i^2 n = n \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} n$\n",
    "- Finally, solve for $\\beta_1$:\n",
    "    - $\\beta_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} n}{\\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} x_i^2 n}$\n",
    "- Finally, substitute equation for $\\beta_1$ into equation for $\\beta_0$:\n",
    "    - $\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\frac{n \\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2} n}{\\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} x_i^2 n} \\sum_{i=1}^{n} x_i - \\frac{\\lambda}{2}}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Lot Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215000</td>\n",
       "      <td>31770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105000</td>\n",
       "      <td>11622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172000</td>\n",
       "      <td>14267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244000</td>\n",
       "      <td>11160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189900</td>\n",
       "      <td>13830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalePrice  Lot Area\n",
       "0     215000     31770\n",
       "1     105000     11622\n",
       "2     172000     14267\n",
       "3     244000     11160\n",
       "4     189900     13830"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('../hw1/AmesHousing.csv')\n",
    "data = data[['SalePrice', 'Lot Area']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function to compute the Ridge estimator from scratch. $\\lambda$ should be a user-input option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression in matrix form:\n",
    "# beta = (X^T X + lambda I)^-1 X^T y\n",
    "\n",
    "def ridge_estimator(X, y, lam):\n",
    "    n, p = X.shape\n",
    "    # add 1s column for intercept\n",
    "    X = np.hstack([np.ones((n, 1)), X])\n",
    "    I = np.eye(p + 1)\n",
    "    betas = np.linalg.inv(X.T @ X + lam * I) @ X.T @ y\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function called `try_many_lambdas()` that takes in a vector or data frame of possible values, and then returns the estimators corresponding to each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function called try_many_lambdas() that takes in a vector or data frame of possible  values, and then returns the estimators corresponding to each value.\n",
    "\n",
    "def try_many_lambdas(lam_values):\n",
    "    X = data[['Lot Area']].values\n",
    "    y = data['SalePrice'].values\n",
    "    betas = {}\n",
    "    for lam in lam_values:\n",
    "        beta = ridge_estimator(X, y, lam)\n",
    "        betas[lam] = beta\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function to compute classical validation metrics for all $\\lambda$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute classical validation metrics for all $\\lambda$'s.\n",
    "# function: tune_lambda_classic\n",
    "# inputs: dataset, set of lambda values, choice of metric\n",
    "    \n",
    "#     for each lambda:\n",
    "        \n",
    "#         compute estimators from data\n",
    "#         get predicted values for data\n",
    "#         computed chosen classic metric\n",
    "\n",
    "# return: data frame of lambdas and corresponding metric\n",
    "\n",
    "# metric options include: r-sq-adjusted, AIC, BIC.\n",
    "# use built-in functions to compute these metrics\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import log\n",
    "\n",
    "def tune_lambda_classic(data, lam_values, metric):\n",
    "    X = data[['Lot Area']].values\n",
    "    y = data['SalePrice'].values\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        beta = ridge_estimator(X, y, lam)\n",
    "        y_pred = beta[0] + beta[1] * X\n",
    "        if metric == 'r-sq-adj':\n",
    "            r2 = r2_score(y, y_pred)\n",
    "            n, p = X.shape\n",
    "            r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "            metrics.append(r2_adj)\n",
    "        elif metric == 'AIC':\n",
    "            n, p = X.shape\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            aic = n * log(mse) + 2 * (p + 1)\n",
    "            metrics.append(aic)\n",
    "        elif metric == 'BIC':\n",
    "            n, p = X.shape\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            bic = n * log(mse) + (p + 1) * log(n)\n",
    "            metrics.append(bic)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run `tune_lambda_classic()` on the AMES data. What was the best choice of $\\lambda$, and the corresponding estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `tune_lambda_classic()` on the AMES data. What was the best choice of $\\lambda$, and the corresponding estimators?\n",
    "\n",
    "lam_values = np.linspace(0, 1000, 100) # 100 values from 0 to 1000\n",
    "\n",
    "df_rsq = tune_lambda_classic(data, lam_values, 'r-sq-adj')\n",
    "df_aic = tune_lambda_classic(data, lam_values, 'AIC')\n",
    "df_bic = tune_lambda_classic(data, lam_values, 'BIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lambda      0.000000\n",
       "r-sq-adj    0.070731\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the highest r-sq-adj\n",
    "df_rsq.loc[df_rsq['r-sq-adj'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lambda        0.000000\n",
       "AIC       65936.878884\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the lowest AIC\n",
    "df_aic.loc[df_aic['AIC'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lambda        0.0000\n",
       "BIC       65948.8444\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the lowest BIC\n",
    "df_bic.loc[df_bic['BIC'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [1.53373893e+05 2.70224462e+00]\n"
     ]
    }
   ],
   "source": [
    "# Get the corresponding estimators for the best choice of $\\lambda$.\n",
    "\n",
    "best_lam = df_rsq.loc[df_rsq['r-sq-adj'].idxmax()]['lambda']\n",
    "betas = try_many_lambdas([best_lam])\n",
    "print(best_lam, betas[best_lam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best choice of $\\lambda$ was 0, and the corresponding estimators were $\\beta_0 = 153373$ and $\\beta_1 = 2.702$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a function to tune your lambda values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write a function to tune your lambda values.\n",
    "\n",
    "# function: tune_lambda_split\n",
    "# inputs: training data, test data, set of lambda values, choice of metric\n",
    "    \n",
    "#     for each lambda:\n",
    "        \n",
    "#         compute estimators on training data\n",
    "#         get predicted values on test data\n",
    "#         computed chosen validation metric\n",
    "\n",
    "# return: data frame of lambdas and corresponding metric\n",
    "\n",
    "# metric options include: r-sq, mse, mae (do not use built-in functions to compute these)\n",
    "\n",
    "def tune_lambda_split(train, test, lam_values, metric):\n",
    "    X_train = train[['Lot Area']].values\n",
    "    y_train = train['SalePrice'].values\n",
    "    X_test = test[['Lot Area']].values\n",
    "    y_test = test['SalePrice'].values\n",
    "    metrics = []\n",
    "    for lam in lam_values:\n",
    "        beta = ridge_estimator(X_train, y_train, lam)\n",
    "        y_pred = beta[0] + beta[1] * X_test\n",
    "        if metric == 'r-sq':\n",
    "            y_bar = np.mean(y_test)\n",
    "            ss_tot = np.sum((y_test - y_bar) ** 2)\n",
    "            ss_res = np.sum((y_test - y_pred) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "            metrics.append(r2)\n",
    "        elif metric == 'mse':\n",
    "            mse = np.mean((y_test - y_pred) ** 2)\n",
    "            metrics.append(mse)\n",
    "        elif metric == 'mae':\n",
    "            mae = np.mean(np.abs(y_test - y_pred))\n",
    "            metrics.append(mae)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metrics})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run your `tune_lambda_split` function on the AMES housing data, for a random 80/20 test/training split, using R-squared as your metric. What was the best choice of $\\lambda$, and the corresponding estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [1.53373893e+05 2.70224462e+00]\n"
     ]
    }
   ],
   "source": [
    "# 6. Run your `tune_lambda_split` function on the AMES housing data, for a random 80/20 test/training split, using R-squared as your metric. What was the best choice of $\\lambda$, and the corresponding estimators?\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "df_rsq = tune_lambda_split(train, test, lam_values, 'r-sq')\n",
    "best_lam = df_rsq.loc[df_rsq['r-sq'].idxmax()]['lambda']\n",
    "betas = try_many_lambdas([best_lam])\n",
    "print(best_lam, betas[best_lam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 20.2020202020202\n"
     ]
    }
   ],
   "source": [
    "# get the best lambda using mse\n",
    "df_mse = tune_lambda_split(train, test, lam_values, 'mse')\n",
    "best_lam_mse = df_mse.loc[df_mse['mse'].idxmin()]['lambda']\n",
    "\n",
    "# get the best lambda using mae\n",
    "df_mae = tune_lambda_split(train, test, lam_values, 'mae')\n",
    "best_lam_mae = df_mae.loc[df_mae['mae'].idxmin()]['lambda']\n",
    "\n",
    "print(best_lam_mse, best_lam_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best choice of $\\lambda$ was 0, and the corresponding estimators were $\\beta_0 = 153373$ and $\\beta_1 = 2.702$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Write a function called `tune_lambda_cv`, which performs v-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7. Write a function called `tune_lambda_cv`, which performs v-fold cross-validation.\n",
    "\n",
    "# function: tune_lambda_cv\n",
    "# inputs: data, set of lambda values, choice of metric, number of splits\n",
    "\n",
    "#     establish v cross validation splits\n",
    "    \n",
    "#     for each split:\n",
    "#         run tune_lambda_split\n",
    "        \n",
    "#     find the average of the metric values for each lambda\n",
    "\n",
    "# return: data frame of lambda and corresponding metric\n",
    "\n",
    "def tune_lambda_cv(data, lam_values, metric, v):\n",
    "    n = len(data)\n",
    "    splits = np.array_split(data, v)\n",
    "    metric_vals = []\n",
    "    for lam in lam_values:\n",
    "        metric_sum = 0\n",
    "        for i in range(v):\n",
    "            # get test and train data based on split\n",
    "            test = splits[i]\n",
    "            train = pd.concat([x for j, x in enumerate(splits) if j != i])\n",
    "            df = tune_lambda_split(train, test, [lam], metric)\n",
    "            metric_sum += df[metric].values[0]\n",
    "        metric_vals.append(metric_sum / v)\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metric_vals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Run your `tune_lambda_cv` function on the AMES housing data with 5 folds using R-squared as your metric. What was the best choice of $\\lambda$, and the corresponding estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [1.53373893e+05 2.70224462e+00]\n"
     ]
    }
   ],
   "source": [
    "df_rsq = tune_lambda_cv(data, lam_values, 'r-sq', 5)\n",
    "best_lam = df_rsq.loc[df_rsq['r-sq'].idxmax()]['lambda']\n",
    "betas = try_many_lambdas([best_lam])\n",
    "print(best_lam, betas[best_lam])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best choice of $\\lambda$ was 0, and the corresponding estimators were $\\beta_0 = 153373$ and $\\beta_1 = 2.702$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write a function called `tune_lambda_loo`, which performs leave-one-out cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a function called `tune_lambda_loo`, which performs leave-one-out cross-validation.\n",
    "\n",
    "# function: tune_lambda_loo\n",
    "# inputs: data, set of lambda values, choice of metric\n",
    "    \n",
    "#     for each lambda:\n",
    "    \n",
    "#         for i in 1 to nrows of data:\n",
    "        \n",
    "#             compute estimators on data without row i\n",
    "#             get predicted value for row i\n",
    "        \n",
    "        \n",
    "#     compute chosen metric across all row predictions\n",
    "\n",
    "# return: data frame of lambda and corresponding metric\n",
    "\n",
    "def tune_lambda_loo(data, lam_values, metric):\n",
    "    n = len(data)\n",
    "    metric_vals = []\n",
    "    \n",
    "    X = data[['Lot Area']].values\n",
    "    y = data['SalePrice'].values\n",
    "\n",
    "    for lam in lam_values:\n",
    "        pred_vals = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            X_train = np.delete(X, i, axis=0)\n",
    "            y_train = np.delete(y, i)\n",
    "\n",
    "            betas = ridge_estimator(X_train, y_train, lam)\n",
    "\n",
    "            pred_vals[i] = betas[0] + betas[1] * X[i]\n",
    "        \n",
    "        # Calculate the chosen metric\n",
    "        if metric == 'r-sq':\n",
    "            y_bar = np.mean(y)\n",
    "            ss_tot = np.sum((y - y_bar) ** 2)\n",
    "            ss_res = np.sum((y - pred_vals) ** 2)\n",
    "            r2 = 1 - ss_res / ss_tot\n",
    "            metric_vals.append(r2)\n",
    "        elif metric == 'mse':\n",
    "            mse = np.mean((y - pred_vals) ** 2)\n",
    "            metric_vals.append(mse)\n",
    "        elif metric == 'mae':\n",
    "            mae = np.mean(np.abs(y - pred_vals))\n",
    "            metric_vals.append(mae)\n",
    "\n",
    "    return pd.DataFrame({'lambda': lam_values, metric: metric_vals})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Run your `tune_lambda_loo` function on the AMES housing data, using R-squared as your metric. What was the best choice of $\\lambda$, and the corresponding estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [1.53373893e+05 2.70224462e+00]\n"
     ]
    }
   ],
   "source": [
    "# 10. Run your `tune_lambda_loo` function on the AMES housing data, using R-squared as your metric. What was the best choice of $\\lambda$, and the corresponding estimators?\n",
    "\n",
    "df_rsq = tune_lambda_loo(data, lam_values, 'r-sq')\n",
    "best_lam = df_rsq.loc[df_rsq['r-sq'].idxmax()]['lambda']\n",
    "betas = try_many_lambdas([best_lam])\n",
    "print(best_lam, betas[best_lam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best choice of $\\lambda$ was 0, and the corresponding estimators were $\\beta_0 = 153373$ and $\\beta_1 = 2.702$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Under what circumstances would your Ridge estimators be equal to 0?\n",
    "- The Ridge estimators would be equal to 0 when the penalty term is greater than the sum of the squared errors. This means when $\\lambda$ becomes really large, the penalty term will dominate the loss function which involves the sum of squared coeffs. This will force the coefficients to start to shrink towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Under what circumstances would your LASSO estimators be equal to 0?\n",
    "- The LASSO estimators can be exactly 0 when $\\lambda$ is large enough. This is because the LASSO penalty term is the sum of the absolute values of the coefficients. At a certain point, the betas would become 0 with a certain value of $\\lambda$, but greater than that the values of betas become negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Consider the simple toy dataset\n",
    "\n",
    "sq footage (in thousands) | 1 | 2 | 3 | 4 |\n",
    "\n",
    "price (in millions) | 1 | 2 | 3 | 14 |\n",
    "\n",
    "Compute the Ridge and LASSO estimators for $\\lambda = 0.1$, $\\lambda = 10$, and $\\lambda = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to compute the lasso estimators from scratch:\n",
    "\n",
    "def lasso_estimator(X, y, lam):\n",
    "    n, p = X.shape\n",
    "    # add 1s column for intercept\n",
    "    X = np.hstack([np.ones((n, 1)), X])\n",
    "    betas = np.zeros(p + 1)\n",
    "    beta_1_numerator = n * np.sum(X[:, 1] * y) - np.sum(y) * np.sum(X[:, 1]) - lam / 2 * np.sum(X[:, 1]) - lam / 2 * n\n",
    "    beta_1_denominator = np.sum(X[:, 1] ** 2) * n + np.sum(X[:, 1]) ** 2\n",
    "    betas[1] = beta_1_numerator / beta_1_denominator\n",
    "    beta_0_numerator = np.sum(y) - beta_1_numerator * np.sum(X[:, 1]) / n - lam / 2\n",
    "    betas[0] = beta_0_numerator / n\n",
    "    return betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.1\n",
      "Ridge: [-4.18624519  3.71636053]\n",
      "Lasso: [-44.575        0.36045455]\n",
      "\n",
      "Lambda: 10\n",
      "Ridge: [0.2173913  1.69565217]\n",
      "Lasso: [-2.5         0.04545455]\n",
      "\n",
      "Lambda: 100\n",
      "Ridge: [0.14157973 0.52757079]\n",
      "Lasso: [380.          -2.81818182]\n"
     ]
    }
   ],
   "source": [
    "# sq footage (in thousands) | 1 | 2 | 3 | 4 |\n",
    "\n",
    "# price (in millions) | 1 | 2 | 3 | 14 |\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 2, 3, 14])\n",
    "\n",
    "# for lambda = 0.1\n",
    "ridge_betas = ridge_estimator(X, y, 0.1)\n",
    "lasso_betas = lasso_estimator(X, y, 0.1)\n",
    "print('Lambda:', 0.1)\n",
    "print('Ridge:', ridge_betas)\n",
    "print('Lasso:', lasso_betas)\n",
    "print()\n",
    "\n",
    "# for lambda = 10\n",
    "ridge_betas = ridge_estimator(X, y, 10)\n",
    "lasso_betas = lasso_estimator(X, y, 10)\n",
    "print('Lambda:', 10)\n",
    "print('Ridge:', ridge_betas)\n",
    "print('Lasso:', lasso_betas)\n",
    "print()\n",
    "\n",
    "# for lambda = 100\n",
    "ridge_betas = ridge_estimator(X, y, 100)\n",
    "lasso_betas = lasso_estimator(X, y, 100)\n",
    "print('Lambda:', 100)\n",
    "print('Ridge:', ridge_betas)\n",
    "print('Lasso:', lasso_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How did your estimators change as $\\lambda$ got bigger for Ridge and LASSO? Why?\n",
    "- As $\\lambda$ got bigger for Ridge and lasso, the slope estimator $\\beta_1$ got smaller. This is because a penalty is being applied to the loss function which forces the coefficients to shrink towards 0. This is especially true for LASSO where the coefficients can become exactly 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. You now observe a new observation: A house that is 5000 square feet and costs $5 million. Which of your six sets of estimators (three lambda values each for Ridge and LASSO) predicted this new value the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge with lambda = 0.1: 14.395557454079452\n",
      "Absolute error: 9.395557454079452\n",
      "Lasso with lambda = 0.1: -42.77272727272727\n",
      "Absolute error: 47.77272727272727\n",
      "\n",
      "Ridge with lambda = 10: 8.695652173913043\n",
      "Absolute error: 3.695652173913043\n",
      "Lasso with lambda = 10: -2.2727272727272725\n",
      "Absolute error: 7.2727272727272725\n",
      "\n",
      "Lasso with lambda = 100: 365.90909090909093\n",
      "Absolute error: 360.90909090909093\n",
      "Lasso with lambda = 100: 365.90909090909093\n",
      "Absolute error: 360.90909090909093\n"
     ]
    }
   ],
   "source": [
    "y_true = 5\n",
    "# ridge with lambda = 0.1\n",
    "ridge_betas = ridge_estimator(X, y, 0.1)\n",
    "y_pred = ridge_betas[0] + ridge_betas[1] * 5\n",
    "print('Ridge with lambda = 0.1:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)\n",
    "\n",
    "# lasso with lambda = 0.1\n",
    "lasso_betas = lasso_estimator(X, y, 0.1)\n",
    "y_pred = lasso_betas[0] + lasso_betas[1] * 5\n",
    "print('Lasso with lambda = 0.1:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)\n",
    "print()\n",
    "\n",
    "# ridge with lambda = 10\n",
    "ridge_betas = ridge_estimator(X, y, 10)\n",
    "y_pred = ridge_betas[0] + ridge_betas[1] * 5\n",
    "print('Ridge with lambda = 10:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)\n",
    "\n",
    "# lasso with lambda = 10\n",
    "lasso_betas = lasso_estimator(X, y, 10)\n",
    "y_pred = lasso_betas[0] + lasso_betas[1] * 5\n",
    "print('Lasso with lambda = 10:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)\n",
    "print()\n",
    "\n",
    "# lasso with lambda = 100\n",
    "lasso_betas = lasso_estimator(X, y, 100)\n",
    "y_pred = lasso_betas[0] + lasso_betas[1] * 5\n",
    "print('Lasso with lambda = 100:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)\n",
    "\n",
    "# lasso with lambda = 100\n",
    "lasso_betas = lasso_estimator(X, y, 100)\n",
    "y_pred = lasso_betas[0] + lasso_betas[1] * 5\n",
    "print('Lasso with lambda = 100:', y_pred)\n",
    "abs_error = np.abs(y_true - y_pred)\n",
    "print('Absolute error:', abs_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge with $\\lambda = 10$ predicted the new value the best with an estimated price of $3.69 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run your `tune_lambda_cv()` function setting the number of splits to $n$ (the number of rows in the AMES dataset) and using R-squared as your metric. Why does it break? How is this different from Leave-One-Out cross-validation?\n",
    "- The below code breaks due to a divide by zero error. This happens because the number of splits is now equal to the number of rows. Due to this, the training set becomes empty and so it breaks. This is different from LOO because LOO only leaves out one observation at a time, so the training set is never empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rsq = tune_lambda_cv(data, lam_values, 'r-sq', len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
